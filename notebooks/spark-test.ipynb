{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "# from pyspark.sql.types import StructType, StructField, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_name = \"My PySpark App1\"\n",
    "master_url = \"spark://spark-master:7077\"\n",
    "# local_ip=\"127.0.0.1\"\n",
    "\n",
    "# # spark.local.ip = helps when multiple network interfaces are present. \n",
    "# #                  The driver must be in the same network as the master and slaves\n",
    "# # spark.driver.host =  same as above. This duality might disappear in a future version \n",
    "conf = SparkConf().setAppName(app_name) \\\n",
    "                  .setMaster(master_url) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                         .config(conf=conf) \\\n",
    "                         .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (1, 1), (2, 0), (3, 1), (4, 0), (5, 1), (6, 0), (7, 1), (8, 0), (9, 1)]\n"
     ]
    }
   ],
   "source": [
    "def mod(x):\n",
    "#     import numpy as np\n",
    "    return (x, np.mod(x, 2))\n",
    "\n",
    "rdd = sc.parallelize(range(1000)).map(mod).take(10)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got Pandas dataframe with 100000 elements\n",
      "              squared\n",
      "0   4.517446045852523\n",
      "1  15.732332013358302\n",
      "2  15.746817237579535\n",
      "3  14.863771555570565\n",
      "4  25.046675513280206\n",
      "5  25.663495878697177\n",
      "6  15.379581295365142\n",
      "7   13.08941529270038\n",
      "8  21.426366942065215\n",
      "9     26.446399128856\n"
     ]
    }
   ],
   "source": [
    "num_elements=100000\n",
    "\n",
    "mu, sigma = 2, 0.5\n",
    "v = np.random.normal(mu, sigma, num_elements)\n",
    "rdd1 = sc.parallelize(v)    \n",
    "def mult(x): return x * np.array([2])\n",
    "rdd2 = rdd1.map(mult).map(lambda x: (float(x),))    \n",
    "\n",
    "schema = StructType([StructField(\"value\", FloatType(), True)])\n",
    "    \n",
    "\n",
    "df1 = spark.createDataFrame(rdd2, schema)\n",
    "df1.registerTempTable(\"test\")  \n",
    "\n",
    "def square(x): return x ** 2\n",
    "\n",
    "spark.udf.register(\"squared\", square)\n",
    "df2 = spark.sql(\"SELECT squared(value) squared FROM test\")    \n",
    "    \n",
    "df= df2.toPandas()\n",
    "\n",
    "print(f\"Got Pandas dataframe with {df.size} elements\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
